{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "keypoint-detection.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11k3bpRqvMz9",
        "colab_type": "text"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "colab_type": "code",
        "id": "eumVCfpDlWwf",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import pandas as pd\n",
        "from PIL import Image, ImageDraw\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas.io.json import json_normalize\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import KFold,train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense,Dropout, Conv2D,Conv2DTranspose, BatchNormalization, Activation,AveragePooling2D,GlobalAveragePooling2D, Input, Concatenate, MaxPool2D, Add, UpSampling2D, LeakyReLU,ZeroPadding2D\n",
        "from keras.models import Model\n",
        "from keras.objectives import mean_squared_error\n",
        "from keras import backend as K\n",
        "from keras.losses import binary_crossentropy\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau,LearningRateScheduler\n",
        "import os  \n",
        "\n",
        "from keras.optimizers import Adam, RMSprop, SGD\n",
        "from tensorflow.compat.v1 import ConfigProto\n",
        "from tensorflow.compat.v1 import InteractiveSession\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "viYi-cjclZ2D",
        "colab": {}
      },
      "source": [
        "path_1=\"../input/train.csv\"\n",
        "path_2=\"../input/train_images/\"\n",
        "path_3=\"../input/test_images/\"\n",
        "path_4=\"../input/sample_submission.csv\"\n",
        "df_train=pd.read_csv(path_1)\n",
        "#print(df_train.head())\n",
        "#print(df_train.shape)\n",
        "df_train=df_train.dropna(axis=0, how='any')#you can use nan data(page with no letter)\n",
        "df_train=df_train.reset_index(drop=True)\n",
        "#print(df_train.shape)\n",
        "\n",
        "annotation_list_train=[]\n",
        "category_names=set()\n",
        "\n",
        "for i in range(len(df_train)):\n",
        "  ann=np.array(df_train.loc[i,\"labels\"].split(\" \")).reshape(-1,5)#cat,x,y,width,height for each picture\n",
        "  category_names=category_names.union({i for i in ann[:,0]})\n",
        "\n",
        "category_names=sorted(category_names)\n",
        "dict_cat={list(category_names)[j]:str(j) for j in range(len(category_names))}\n",
        "inv_dict_cat={str(j):list(category_names)[j] for j in range(len(category_names))}\n",
        "#print(dict_cat)\n",
        "  \n",
        "for i in range(len(df_train)):\n",
        "  ann=np.array(df_train.loc[i,\"labels\"].split(\" \")).reshape(-1,5)#cat,left,top,width,height for each picture\n",
        "  for j,category_name in enumerate(ann[:,0]):\n",
        "    ann[j,0]=int(dict_cat[category_name])  \n",
        "  ann=ann.astype('int32')\n",
        "  ann[:,1]+=ann[:,3]//2#center_x\n",
        "  ann[:,2]+=ann[:,4]//2#center_y\n",
        "  annotation_list_train.append([\"{}{}.jpg\".format(path_2,df_train.loc[i,\"image_id\"]),ann])\n",
        "\n",
        "print(\"sample image\")\n",
        "input_width,input_height=512, 512\n",
        "img = np.asarray(Image.open(annotation_list_train[0][0]).resize((input_width,input_height)).convert('RGB'))\n",
        "plt.imshow(img)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KtKhxahpqXLZ",
        "colab": {}
      },
      "source": [
        "# get directory of test images\n",
        "df_submission=pd.read_csv(path_4)\n",
        "id_test=path_3+df_submission[\"image_id\"]+\".jpg\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dlTW8tO9R7vJ"
      },
      "source": [
        "## STEP 1: Preprocessing (Check Object Size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "p1066wEtR_g_"
      },
      "source": [
        "**Goal of Step1 is to check the object size and determine the size of input image for detector.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "colab_type": "code",
        "id": "rP6_99JH6zoU",
        "colab": {}
      },
      "source": [
        "aspect_ratio_pic_all=[]\n",
        "aspect_ratio_pic_all_test=[]\n",
        "average_letter_size_all=[]\n",
        "train_input_for_size_estimate=[]\n",
        "resize_dir=\"resized/\"\n",
        "if os.path.exists(resize_dir) == False:os.mkdir(resize_dir)\n",
        "for i in range(len(annotation_list_train)):\n",
        "    with Image.open(annotation_list_train[i][0]) as f:\n",
        "        width,height=f.size\n",
        "        area=width*height\n",
        "        aspect_ratio_pic=height/width\n",
        "        aspect_ratio_pic_all.append(aspect_ratio_pic)\n",
        "        letter_size=annotation_list_train[i][1][:,3]*annotation_list_train[i][1][:,4]\n",
        "        letter_size_ratio=letter_size/area\n",
        "    \n",
        "        average_letter_size=np.mean(letter_size_ratio)\n",
        "        average_letter_size_all.append(average_letter_size)\n",
        "        train_input_for_size_estimate.append([annotation_list_train[i][0],np.log(average_letter_size)])#logにしとく\n",
        "    \n",
        "\n",
        "for i in range(len(id_test)):\n",
        "    with Image.open(id_test[i]) as f:\n",
        "        width,height=f.size\n",
        "        aspect_ratio_pic=height/width\n",
        "        aspect_ratio_pic_all_test.append(aspect_ratio_pic)\n",
        "\n",
        "\n",
        "plt.hist(np.log(average_letter_size_all),bins=100)\n",
        "plt.title('log(ratio of letter_size to picture_size))',loc='center',fontsize=12)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QcTNHyXZJqZY"
      },
      "source": [
        "You can see the average size of the objects varies among pictures. So it would be better to split the picture into several parts. So cropping pictures with appropriate ratio would be important when detecting letters.  One way to find the best cropping size is creating the model to predict the average letter size.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t_Vq2P0BUAkn"
      },
      "source": [
        "### Create Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wcmN-Yu4P-9i"
      },
      "source": [
        "Let's create CNN model(almost ResNet).\n",
        "\n",
        "*   Input: Image (resized into 512x512x3)\n",
        "*   Output: Ratio of letter_size to picture_size\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jVix-CRbIXPf",
        "colab": {}
      },
      "source": [
        "\n",
        "category_n=1\n",
        "import cv2\n",
        "input_width,input_height=512, 512\n",
        "\n",
        "def Datagen_sizecheck_model(filenames, batch_size, size_detection_mode=True, is_train=True,random_crop=True):\n",
        "  x=[]\n",
        "  y=[]\n",
        "  \n",
        "  count=0\n",
        "\n",
        "  while True:\n",
        "    for i in range(len(filenames)):\n",
        "      if random_crop:\n",
        "        crop_ratio=np.random.uniform(0.7,1)\n",
        "      else:\n",
        "        crop_ratio=1\n",
        "      with Image.open(filenames[i][0]) as f:\n",
        "        #random crop\n",
        "        if random_crop and is_train:\n",
        "          pic_width,pic_height=f.size\n",
        "          f=np.asarray(f.convert('RGB'),dtype=np.uint8)\n",
        "          top_offset=np.random.randint(0,pic_height-int(crop_ratio*pic_height))\n",
        "          left_offset=np.random.randint(0,pic_width-int(crop_ratio*pic_width))\n",
        "          bottom_offset=top_offset+int(crop_ratio*pic_height)\n",
        "          right_offset=left_offset+int(crop_ratio*pic_width)\n",
        "          f=cv2.resize(f[top_offset:bottom_offset,left_offset:right_offset,:],(input_height,input_width))\n",
        "        else:\n",
        "          f=f.resize((input_width, input_height))\n",
        "          f=np.asarray(f.convert('RGB'),dtype=np.uint8)          \n",
        "        x.append(f)\n",
        "      \n",
        "      \n",
        "      if random_crop and is_train:\n",
        "        y.append(filenames[i][1]-np.log(crop_ratio))\n",
        "      else:\n",
        "        y.append(filenames[i][1])\n",
        "      \n",
        "      count+=1\n",
        "      if count==batch_size:\n",
        "        x=np.array(x, dtype=np.float32)\n",
        "        y=np.array(y, dtype=np.float32)\n",
        "\n",
        "        inputs=x/255\n",
        "        targets=y       \n",
        "        x=[]\n",
        "        y=[]\n",
        "        count=0\n",
        "        yield inputs, targets\n",
        "\n",
        "\n",
        "\n",
        "def aggregation_block(x_shallow, x_deep, deep_ch, out_ch):\n",
        "  x_deep= Conv2DTranspose(deep_ch, kernel_size=2, strides=2, padding='same', use_bias=False)(x_deep)\n",
        "  x_deep = BatchNormalization()(x_deep)   \n",
        "  x_deep = LeakyReLU(alpha=0.1)(x_deep)\n",
        "  x = Concatenate()([x_shallow, x_deep])\n",
        "  x=Conv2D(out_ch, kernel_size=1, strides=1, padding=\"same\")(x)\n",
        "  x = BatchNormalization()(x)   \n",
        "  x = LeakyReLU(alpha=0.1)(x)\n",
        "  return x\n",
        "  \n",
        "\n",
        "\n",
        "def cbr(x, out_layer, kernel, stride):\n",
        "  x=Conv2D(out_layer, kernel_size=kernel, strides=stride, padding=\"same\")(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = LeakyReLU(alpha=0.1)(x)\n",
        "  return x\n",
        "\n",
        "def resblock(x_in,layer_n):\n",
        "  x=cbr(x_in,layer_n,3,1)\n",
        "  x=cbr(x,layer_n,3,1)\n",
        "  x=Add()([x,x_in])\n",
        "  return x  \n",
        "\n",
        "\n",
        "#I use the same network at CenterNet\n",
        "def create_model(input_shape, size_detection_mode=True, aggregation=True):\n",
        "    input_layer = Input(input_shape)\n",
        "    \n",
        "    #resized input\n",
        "    input_layer_1=AveragePooling2D(2)(input_layer)\n",
        "    input_layer_2=AveragePooling2D(2)(input_layer_1)\n",
        "\n",
        "    #### ENCODER ####\n",
        "\n",
        "    x_0= cbr(input_layer, 16, 3, 2)#512->256\n",
        "    concat_1 = Concatenate()([x_0, input_layer_1])\n",
        "\n",
        "    x_1= cbr(concat_1, 32, 3, 2)#256->128\n",
        "    concat_2 = Concatenate()([x_1, input_layer_2])\n",
        "\n",
        "    x_2= cbr(concat_2, 64, 3, 2)#128->64\n",
        "    \n",
        "    x=cbr(x_2,64,3,1)\n",
        "    x=resblock(x,64)\n",
        "    x=resblock(x,64)\n",
        "    \n",
        "    x_3= cbr(x, 128, 3, 2)#64->32\n",
        "    x= cbr(x_3, 128, 3, 1)\n",
        "    x=resblock(x,128)\n",
        "    x=resblock(x,128)\n",
        "    x=resblock(x,128)\n",
        "    \n",
        "    x_4= cbr(x, 256, 3, 2)#32->16\n",
        "    x= cbr(x_4, 256, 3, 1)\n",
        "    x=resblock(x,256)\n",
        "    x=resblock(x,256)\n",
        "    x=resblock(x,256)\n",
        "    x=resblock(x,256)\n",
        "    x=resblock(x,256)\n",
        " \n",
        "    x_5= cbr(x, 512, 3, 2)#16->8\n",
        "    x= cbr(x_5, 512, 3, 1)\n",
        "    \n",
        "    x=resblock(x,512)\n",
        "    x=resblock(x,512)\n",
        "    x=resblock(x,512)\n",
        "    \n",
        "    if size_detection_mode:\n",
        "      x=GlobalAveragePooling2D()(x)\n",
        "      x=Dropout(0.2)(x)\n",
        "      out=Dense(1,activation=\"linear\")(x)\n",
        "    \n",
        "    else:#centernet mode\n",
        "    #### DECODER ####\n",
        "      x_1= cbr(x_1, output_layer_n, 1, 1)\n",
        "      x_1 = aggregation_block(x_1, x_2, output_layer_n, output_layer_n)\n",
        "      x_2= cbr(x_2, output_layer_n, 1, 1)\n",
        "      x_2 = aggregation_block(x_2, x_3, output_layer_n, output_layer_n)\n",
        "      x_1 = aggregation_block(x_1, x_2, output_layer_n, output_layer_n)\n",
        "      x_3= cbr(x_3, output_layer_n, 1, 1)\n",
        "      x_3 = aggregation_block(x_3, x_4, output_layer_n, output_layer_n) \n",
        "      x_2 = aggregation_block(x_2, x_3, output_layer_n, output_layer_n)\n",
        "      x_1 = aggregation_block(x_1, x_2, output_layer_n, output_layer_n)\n",
        "      \n",
        "      x_4= cbr(x_4, output_layer_n, 1, 1)\n",
        "\n",
        "      x=cbr(x, output_layer_n, 1, 1)\n",
        "      x= UpSampling2D(size=(2, 2))(x)#8->16 tconvのがいいか\n",
        "\n",
        "      x = Concatenate()([x, x_4])\n",
        "      x=cbr(x, output_layer_n, 3, 1)\n",
        "      x= UpSampling2D(size=(2, 2))(x)#16->32\n",
        "    \n",
        "      x = Concatenate()([x, x_3])\n",
        "      x=cbr(x, output_layer_n, 3, 1)\n",
        "      x= UpSampling2D(size=(2, 2))(x)#32->64   128のがいいかも？ \n",
        "    \n",
        "      x = Concatenate()([x, x_2])\n",
        "      x=cbr(x, output_layer_n, 3, 1)\n",
        "      x= UpSampling2D(size=(2, 2))(x)#64->128 \n",
        "      \n",
        "      x = Concatenate()([x, x_1])\n",
        "      x=Conv2D(output_layer_n, kernel_size=3, strides=1, padding=\"same\")(x)\n",
        "      out = Activation(\"sigmoid\")(x)\n",
        "    \n",
        "    model=Model(input_layer, out)\n",
        "    \n",
        "    return model\n",
        "  \n",
        "    \n",
        "\n",
        "\n",
        "def model_fit_sizecheck_model(model,train_list,cv_list,n_epoch,batch_size=32):\n",
        "    hist = model.fit_generator(\n",
        "        Datagen_sizecheck_model(train_list,batch_size, is_train=True,random_crop=True),\n",
        "        steps_per_epoch = len(train_list) // batch_size,\n",
        "        epochs = n_epoch,\n",
        "        validation_data=Datagen_sizecheck_model(cv_list,batch_size, is_train=False,random_crop=False),\n",
        "        validation_steps = len(cv_list) // batch_size,\n",
        "        callbacks = [lr_schedule, model_checkpoint],#[early_stopping, reduce_lr, model_checkpoint],\n",
        "        shuffle = True,\n",
        "        verbose = 1\n",
        "    )\n",
        "    return hist\n",
        "\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-output": true,
        "colab_type": "code",
        "id": "1F1a3C_LW_UY",
        "colab": {}
      },
      "source": [
        "K.clear_session()\n",
        "model=create_model(input_shape=(input_height,input_width,3),size_detection_mode=True)\n",
        "\"\"\"\n",
        "# EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor = 'val_loss', min_delta=0, patience = 10, verbose = 1)\n",
        "# ModelCheckpoint\n",
        "weights_dir = '/model_1/'\n",
        "if os.path.exists(weights_dir) == False:os.mkdir(weights_dir)\n",
        "model_checkpoint = ModelCheckpoint(weights_dir + \"val_loss{val_loss:.3f}.hdf5\", monitor = 'val_loss', verbose = 1,\n",
        "                                      save_best_only = True, save_weights_only = True, period = 1)\n",
        "# reduce learning rate\n",
        "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.5, patience = 10, verbose = 1)\n",
        "\"\"\"\n",
        "def lrs(epoch):\n",
        "    lr = 0.0005\n",
        "    if epoch>10:\n",
        "        lr = 0.0001\n",
        "    return lr\n",
        "\n",
        "lr_schedule = LearningRateScheduler(lrs)\n",
        "model_checkpoint = ModelCheckpoint(\"final_weights_step1.hdf5\", monitor = 'val_loss', verbose = 1,\n",
        "                                      save_best_only = True, save_weights_only = True, period = 1)\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1UMVc89HSNNO"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7126xqIkoI-Q"
      },
      "source": [
        "Step1 is not main topic of this kernel. Run only 15 epochs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5W6-Twl9Z6TR",
        "colab": {}
      },
      "source": [
        "train_list, cv_list = train_test_split(train_input_for_size_estimate, random_state = 111,test_size = 0.2)\n",
        "\n",
        "\n",
        "learning_rate=0.0005\n",
        "n_epoch=12\n",
        "batch_size=32\n",
        "\n",
        "model.compile(loss=mean_squared_error, optimizer=Adam(lr=learning_rate))\n",
        "hist = model_fit_sizecheck_model(model,train_list,cv_list,n_epoch,batch_size)\n",
        "\n",
        "#model.save_weights('final_weights_step1.h5')\n",
        "model.load_weights('final_weights_step1.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-rpgdUdOSXtC"
      },
      "source": [
        "### Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DNonHXQ7fEC8",
        "colab": {}
      },
      "source": [
        "predict = model.predict_generator(Datagen_sizecheck_model(cv_list,batch_size, is_train=False,random_crop=False),\n",
        "                                  steps=len(cv_list) // batch_size)\n",
        "target=[cv[1] for cv in cv_list]\n",
        "plt.scatter(predict,target[:len(predict)])\n",
        "plt.title('---letter_size/picture_size--- estimated vs target ',loc='center',fontsize=10)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Q2J5SM-QS88c"
      },
      "source": [
        "Umm... not so good. Training is not enough. But I might as well use it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pTyR0fO1oZuz",
        "colab": {}
      },
      "source": [
        "batch_size=1\n",
        "predict_train = model.predict_generator(Datagen_sizecheck_model(train_input_for_size_estimate,batch_size, is_train=False,random_crop=False, ),\n",
        "                                  steps=len(train_input_for_size_estimate)//batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vvTtQpu3UzR_"
      },
      "source": [
        "Based on the detector's size, split numbers are determined for each picture. In the next section I will make the CenterNet whose output shape is 128x128. Assuming the letters are detected every 5 pixcels, this detector can find the letters 25x25 at most.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uiW8NoOL8kV4",
        "colab": {}
      },
      "source": [
        "base_detect_num_h,base_detect_num_w=25,25\n",
        "annotation_list_train_w_split=[]\n",
        "for i, predicted_size in enumerate(predict_train):\n",
        "  detect_num_h=aspect_ratio_pic_all[i]*np.exp(-predicted_size/2)\n",
        "  detect_num_w=detect_num_h/aspect_ratio_pic_all[i]\n",
        "  h_split_recommend=np.maximum(1,detect_num_h/base_detect_num_h)\n",
        "  w_split_recommend=np.maximum(1,detect_num_w/base_detect_num_w)\n",
        "  annotation_list_train_w_split.append([annotation_list_train[i][0],annotation_list_train[i][1],h_split_recommend,w_split_recommend])\n",
        "for i in np.arange(0,1):\n",
        "  print(\"recommended height split:{}, recommended width_split:{}\".format(annotation_list_train_w_split[i][2],annotation_list_train_w_split[i][3]))\n",
        "  img = np.asarray(Image.open(annotation_list_train_w_split[i][0]).convert('RGB'))\n",
        "  plt.imshow(img)\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Fuh4HivsWq98"
      },
      "source": [
        "## STEP 2: Detection by CenterNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4P6dGIVNWrOL"
      },
      "source": [
        "**Goal of stage2 is to detect the letters by CenterNet.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mk0t0a6-XGmY"
      },
      "source": [
        "I'd like to change the generator and model. The differences from step_1 are as follows:\n",
        "\n",
        "\n",
        "*   Add random cropping to generator\n",
        "*  Make target data for CenterNet\n",
        "*   Add decoder to the network (similar to U-Net)\n",
        "\n",
        "**Input of CenterNet:** \n",
        "\n",
        ">Cropped image resized into 512x512x3\n",
        "\n",
        "\n",
        "**Output of CenterNet:** \n",
        "\n",
        ">Heatmap of center point 128x128x1\n",
        "\n",
        ">x-offset and y-offset of the center point inside the detected cell 128x128x2\n",
        "\n",
        ">width and height of the the detected object 128x128x2\n",
        "\n",
        "It should be noted that this network can cope with multiple categories by increasing the number of output layers of center point. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kc0KaTAvM2W",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z3wxB7tzUd3V"
      },
      "source": [
        "### Create Model & Training Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YxqBA229TQJy"
      },
      "source": [
        "Before creating the model, I'd like to show the example image of target heatmap showing center points of objects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "colab_type": "code",
        "id": "hAB45SxiS4Sg",
        "colab": {}
      },
      "source": [
        "category_n=1\n",
        "output_layer_n=category_n+4\n",
        "output_height,output_width=128,128\n",
        "\n",
        "i=0\n",
        "\n",
        "h_split=annotation_list_train_w_split[i][2]\n",
        "w_split=annotation_list_train_w_split[i][3]\n",
        "max_crop_ratio_h=1/h_split\n",
        "max_crop_ratio_w=1/w_split\n",
        "crop_ratio=np.random.uniform(0.5,1)\n",
        "crop_ratio_h=max_crop_ratio_h*crop_ratio\n",
        "crop_ratio_w=max_crop_ratio_w*crop_ratio\n",
        "\n",
        "with Image.open(annotation_list_train_w_split[i][0]) as f:\n",
        "        \n",
        "        #random crop\n",
        "        pic_width,pic_height=f.size\n",
        "        f=np.asarray(f.convert('RGB'),dtype=np.uint8)\n",
        "        top_offset=np.random.randint(0,pic_height-int(crop_ratio_h*pic_height))\n",
        "        left_offset=np.random.randint(0,pic_width-int(crop_ratio_w*pic_width))\n",
        "        bottom_offset=top_offset+int(crop_ratio_h*pic_height)\n",
        "        right_offset=left_offset+int(crop_ratio_w*pic_width)\n",
        "        img=cv2.resize(f[top_offset:bottom_offset,left_offset:right_offset,:],(input_height,input_width))\n",
        "\n",
        "      \n",
        "      \n",
        "output_layer=np.zeros((output_height,output_width,(output_layer_n+category_n)))\n",
        "for annotation in annotation_list_train_w_split[i][1]:\n",
        "\n",
        "          x_c=(annotation[1]-left_offset)*(output_width/int(crop_ratio_w*pic_width))\n",
        "          y_c=(annotation[2]-top_offset)*(output_height/int(crop_ratio_h*pic_height))\n",
        "          width=annotation[3]*(output_width/int(crop_ratio_w*pic_width))\n",
        "          height=annotation[4]*(output_height/int(crop_ratio_h*pic_height))\n",
        "          \n",
        "          top=np.maximum(0,y_c-height/2)\n",
        "          left=np.maximum(0,x_c-width/2)\n",
        "          bottom=np.minimum(output_height,y_c+height/2)\n",
        "          right=np.minimum(output_width,x_c+width/2)\n",
        "          \n",
        "          if top>=output_height or left>=output_width or bottom<=0 or right<=0:#random crop(エリア外の除去)\n",
        "            continue\n",
        "          width=right-left\n",
        "          height=bottom-top\n",
        "          x_c=(right+left)/2\n",
        "          y_c=(top+bottom)/2\n",
        "          \n",
        "        \n",
        "        \n",
        "          category=0#not classify\n",
        "          heatmap=((np.exp(-(((np.arange(output_width)-x_c)/(width/10))**2)/2)).reshape(1,-1)\n",
        "                            *(np.exp(-(((np.arange(output_height)-y_c)/(height/10))**2)/2)).reshape(-1,1))\n",
        "          output_layer[:,:,category]=np.maximum(output_layer[:,:,category],heatmap[:,:])\n",
        "          output_layer[int(y_c//1),int(x_c//1),category_n+category]=1\n",
        "          output_layer[int(y_c//1),int(x_c//1),2*category_n]=y_c%1#height offset\n",
        "          output_layer[int(y_c//1),int(x_c//1),2*category_n+1]=x_c%1\n",
        "          output_layer[int(y_c//1),int(x_c//1),2*category_n+2]=height/output_height\n",
        "          output_layer[int(y_c//1),int(x_c//1),2*category_n+3]=width/output_width\n",
        "\n",
        "fig, axes = plt.subplots(1, 3,figsize=(15,15))\n",
        "axes[0].set_axis_off()\n",
        "axes[0].imshow(img)\n",
        "axes[1].set_axis_off()\n",
        "axes[1].imshow(output_layer[:,:,1])\n",
        "axes[2].set_axis_off()\n",
        "axes[2].imshow(output_layer[:,:,0])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HtFsORI-E3CB"
      },
      "source": [
        "- Left image is the original picture(cropped).\n",
        "\n",
        "- Middle one is the target image in which center points are 1, other pixcels are 0.\n",
        "\n",
        "- Right one shows center points with gaussian distributions.\n",
        "\n",
        "Right image is necessary to reduce the training loss when the model detect the points near the exact center. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "30EkxAvQ-XkH",
        "colab": {}
      },
      "source": [
        "category_n=1\n",
        "output_layer_n=category_n+4\n",
        "output_height,output_width=128,128\n",
        "\n",
        "def Datagen_centernet(filenames, batch_size):\n",
        "  x=[]\n",
        "  y=[]\n",
        "  \n",
        "  count=0\n",
        "\n",
        "  while True:\n",
        "    for i in range(len(filenames)):\n",
        "      h_split=filenames[i][2]\n",
        "      w_split=filenames[i][3]\n",
        "      max_crop_ratio_h=1/h_split\n",
        "      max_crop_ratio_w=1/w_split\n",
        "      crop_ratio=np.random.uniform(0.5,1)\n",
        "      crop_ratio_h=max_crop_ratio_h*crop_ratio\n",
        "      crop_ratio_w=max_crop_ratio_w*crop_ratio\n",
        "      \n",
        "      with Image.open(filenames[i][0]) as f:\n",
        "        \n",
        "        #random crop\n",
        "        \n",
        "        pic_width,pic_height=f.size\n",
        "        f=np.asarray(f.convert('RGB'),dtype=np.uint8)\n",
        "        top_offset=np.random.randint(0,pic_height-int(crop_ratio_h*pic_height))\n",
        "        left_offset=np.random.randint(0,pic_width-int(crop_ratio_w*pic_width))\n",
        "        bottom_offset=top_offset+int(crop_ratio_h*pic_height)\n",
        "        right_offset=left_offset+int(crop_ratio_w*pic_width)\n",
        "        f=cv2.resize(f[top_offset:bottom_offset,left_offset:right_offset,:],(input_height,input_width))\n",
        "        x.append(f)      \n",
        "\n",
        "      output_layer=np.zeros((output_height,output_width,(output_layer_n+category_n)))\n",
        "      for annotation in filenames[i][1]:\n",
        "        x_c=(annotation[1]-left_offset)*(output_width/int(crop_ratio_w*pic_width))\n",
        "        y_c=(annotation[2]-top_offset)*(output_height/int(crop_ratio_h*pic_height))\n",
        "        width=annotation[3]*(output_width/int(crop_ratio_w*pic_width))\n",
        "        height=annotation[4]*(output_height/int(crop_ratio_h*pic_height))\n",
        "        top=np.maximum(0,y_c-height/2)\n",
        "        left=np.maximum(0,x_c-width/2)\n",
        "        bottom=np.minimum(output_height,y_c+height/2)\n",
        "        right=np.minimum(output_width,x_c+width/2)\n",
        "          \n",
        "        if top>=(output_height-0.1) or left>=(output_width-0.1) or bottom<=0.1 or right<=0.1:#random crop(out of picture)\n",
        "          continue\n",
        "        width=right-left\n",
        "        height=bottom-top\n",
        "        x_c=(right+left)/2\n",
        "        y_c=(top+bottom)/2\n",
        "\n",
        "        \n",
        "        category=0#not classify, just detect\n",
        "        heatmap=((np.exp(-(((np.arange(output_width)-x_c)/(width/10))**2)/2)).reshape(1,-1)\n",
        "                            *(np.exp(-(((np.arange(output_height)-y_c)/(height/10))**2)/2)).reshape(-1,1))\n",
        "        output_layer[:,:,category]=np.maximum(output_layer[:,:,category],heatmap[:,:])\n",
        "        output_layer[int(y_c//1),int(x_c//1),category_n+category]=1\n",
        "        output_layer[int(y_c//1),int(x_c//1),2*category_n]=y_c%1#height offset\n",
        "        output_layer[int(y_c//1),int(x_c//1),2*category_n+1]=x_c%1\n",
        "        output_layer[int(y_c//1),int(x_c//1),2*category_n+2]=height/output_height\n",
        "        output_layer[int(y_c//1),int(x_c//1),2*category_n+3]=width/output_width\n",
        "      y.append(output_layer)  \n",
        "    \n",
        "      count+=1\n",
        "      if count==batch_size:\n",
        "        x=np.array(x, dtype=np.float32)\n",
        "        y=np.array(y, dtype=np.float32)\n",
        "\n",
        "        inputs=x/255\n",
        "        targets=y       \n",
        "        x=[]\n",
        "        y=[]\n",
        "        count=0\n",
        "        yield inputs, targets\n",
        "\n",
        "def all_loss(y_true, y_pred):\n",
        "    mask=K.sign(y_true[...,2*category_n+2])\n",
        "    N=K.sum(mask)\n",
        "    alpha=2.\n",
        "    beta=4.\n",
        "\n",
        "    heatmap_true_rate = K.flatten(y_true[...,:category_n])\n",
        "    heatmap_true = K.flatten(y_true[...,category_n:(2*category_n)])\n",
        "    heatmap_pred = K.flatten(y_pred[...,:category_n])\n",
        "    heatloss=-K.sum(heatmap_true*((1-heatmap_pred)**alpha)*K.log(heatmap_pred+1e-6)+(1-heatmap_true)*((1-heatmap_true_rate)**beta)*(heatmap_pred**alpha)*K.log(1-heatmap_pred+1e-6))\n",
        "    offsetloss=K.sum(K.abs(y_true[...,2*category_n]-y_pred[...,category_n]*mask)+K.abs(y_true[...,2*category_n+1]-y_pred[...,category_n+1]*mask))\n",
        "    sizeloss=K.sum(K.abs(y_true[...,2*category_n+2]-y_pred[...,category_n+2]*mask)+K.abs(y_true[...,2*category_n+3]-y_pred[...,category_n+3]*mask))\n",
        "    \n",
        "    all_loss=(heatloss+1.0*offsetloss+5.0*sizeloss)/N\n",
        "    return all_loss\n",
        "\n",
        "def size_loss(y_true, y_pred):\n",
        "    mask=K.sign(y_true[...,2*category_n+2])\n",
        "    N=K.sum(mask)\n",
        "    sizeloss=K.sum(K.abs(y_true[...,2*category_n+2]-y_pred[...,category_n+2]*mask)+K.abs(y_true[...,2*category_n+3]-y_pred[...,category_n+3]*mask))\n",
        "    return (5*sizeloss)/N\n",
        "\n",
        "def offset_loss(y_true, y_pred):\n",
        "    mask=K.sign(y_true[...,2*category_n+2])\n",
        "    N=K.sum(mask)\n",
        "    offsetloss=K.sum(K.abs(y_true[...,2*category_n]-y_pred[...,category_n]*mask)+K.abs(y_true[...,2*category_n+1]-y_pred[...,category_n+1]*mask))\n",
        "    return (offsetloss)/N\n",
        "  \n",
        "def heatmap_loss(y_true, y_pred):\n",
        "    mask=K.sign(y_true[...,2*category_n+2])\n",
        "    N=K.sum(mask)\n",
        "    alpha=2.\n",
        "    beta=4.\n",
        "\n",
        "    heatmap_true_rate = K.flatten(y_true[...,:category_n])\n",
        "    heatmap_true = K.flatten(y_true[...,category_n:(2*category_n)])\n",
        "    heatmap_pred = K.flatten(y_pred[...,:category_n])\n",
        "    heatloss=-K.sum(heatmap_true*((1-heatmap_pred)**alpha)*K.log(heatmap_pred+1e-6)+(1-heatmap_true)*((1-heatmap_true_rate)**beta)*(heatmap_pred**alpha)*K.log(1-heatmap_pred+1e-6))\n",
        "    return heatloss/N\n",
        "\n",
        "  \n",
        "def model_fit_centernet(model,train_list,cv_list,n_epoch,batch_size=32):\n",
        "    hist = model.fit_generator(\n",
        "        Datagen_centernet(train_list,batch_size),\n",
        "        steps_per_epoch = len(train_list) // batch_size,\n",
        "        epochs = n_epoch,\n",
        "        validation_data=Datagen_centernet(cv_list,batch_size),\n",
        "        validation_steps = len(cv_list) // batch_size,\n",
        "        callbacks = [lr_schedule],#early_stopping, reduce_lr, model_checkpoint],\n",
        "        shuffle = True,\n",
        "        verbose = 1\n",
        "    )\n",
        "    return hist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-output": true,
        "colab_type": "code",
        "id": "bJsSWwLCMEua",
        "colab": {}
      },
      "source": [
        "K.clear_session()\n",
        "model=create_model(input_shape=(input_height,input_width,3),size_detection_mode=False)\n",
        "\n",
        "def lrs(epoch):\n",
        "    lr = 0.001\n",
        "    if epoch >= 20: lr = 0.0002\n",
        "    return lr\n",
        "\n",
        "lr_schedule = LearningRateScheduler(lrs)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor = 'val_loss', min_delta=0, patience = 60, verbose = 1)\n",
        "# ModelCheckpoint\n",
        "weights_dir = '/model_2/'\n",
        "\n",
        "if os.path.exists(weights_dir) == False:os.mkdir(weights_dir)\n",
        "model_checkpoint = ModelCheckpoint(weights_dir + \"val_loss{val_loss:.3f}.hdf5\", monitor = 'val_loss', verbose = 1,\n",
        "                                      save_best_only = True, save_weights_only = True, period = 3)\n",
        "# reduce learning rate\n",
        "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.5, patience = 10, verbose = 1)\n",
        "\"\"\"\n",
        "model.load_weights('final_weights_step1.h5',by_name=True, skip_mismatch=True)\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oxjOmJgKUyCQ"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlsQ-wE5vM3A",
        "colab_type": "text"
      },
      "source": [
        "Start training. This kernel runs 30 epochs. Longer training can improve the accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xjVS-w10MMfZ",
        "colab": {}
      },
      "source": [
        "train_list, cv_list = train_test_split(annotation_list_train_w_split, random_state = 111,test_size = 0.2)#stratified split is better\n",
        "\n",
        "learning_rate=0.001\n",
        "n_epoch=30\n",
        "batch_size=32\n",
        "model.compile(loss=all_loss, optimizer=Adam(lr=learning_rate), metrics=[heatmap_loss,size_loss,offset_loss])\n",
        "hist = model_fit_centernet(model,train_list,cv_list,n_epoch,batch_size)\n",
        "\n",
        "model.save_weights('final_weights_step2.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EaB7Y5aUVM1w"
      },
      "source": [
        "### Result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uwzWlysgVT7U"
      },
      "source": [
        "Let's check a result of heatmap with validation data. You can see the centers of letters are detected.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "colab_type": "code",
        "id": "mexdIy9KMQjY",
        "colab": {}
      },
      "source": [
        "pred_in_h=512\n",
        "pred_in_w=512\n",
        "pred_out_h=int(pred_in_h/4)\n",
        "pred_out_w=int(pred_in_w/4)\n",
        "\n",
        "for i in np.arange(0,1):\n",
        "  img = np.asarray(Image.open(cv_list[i][0]).resize((pred_in_w,pred_in_h)).convert('RGB'))\n",
        "  predict=model.predict((img.reshape(1,pred_in_h,pred_in_w,3))/255).reshape(pred_out_h,pred_out_w,(category_n+4))\n",
        "  heatmap=predict[:,:,0]\n",
        "\n",
        "  fig, axes = plt.subplots(1, 2,figsize=(15,15))\n",
        "  axes[0].set_axis_off()\n",
        "  axes[0].imshow(img)\n",
        "  axes[1].set_axis_off()\n",
        "  axes[1].imshow(heatmap)\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RZ4C1UM-Wfug"
      },
      "source": [
        "Then, let's convert the outputs into bounding boxes. I use NMS (Non Maximum Suppression) to find the best boxes. \n",
        "(The original paper says that the max pooling also works well instead of NMS.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "colab_type": "code",
        "id": "M9o_2oqHsZ2N",
        "colab": {}
      },
      "source": [
        "from PIL import Image, ImageDraw\n",
        "\n",
        "def NMS_all(predicts,category_n,score_thresh,iou_thresh):\n",
        "  y_c=predicts[...,category_n]+np.arange(pred_out_h).reshape(-1,1)\n",
        "  x_c=predicts[...,category_n+1]+np.arange(pred_out_w).reshape(1,-1)\n",
        "  height=predicts[...,category_n+2]*pred_out_h\n",
        "  width=predicts[...,category_n+3]*pred_out_w\n",
        "\n",
        "  count=0\n",
        "  for category in range(category_n):\n",
        "    predict=predicts[...,category]\n",
        "    mask=(predict>score_thresh)\n",
        "    #print(\"box_num\",np.sum(mask))\n",
        "    if mask.all==False:\n",
        "      continue\n",
        "    box_and_score=NMS(predict[mask],y_c[mask],x_c[mask],height[mask],width[mask],iou_thresh)\n",
        "    box_and_score=np.insert(box_and_score,0,category,axis=1)#category,score,top,left,bottom,right\n",
        "    if count==0:\n",
        "      box_and_score_all=box_and_score\n",
        "    else:\n",
        "      box_and_score_all=np.concatenate((box_and_score_all,box_and_score),axis=0)\n",
        "    count+=1\n",
        "  score_sort=np.argsort(box_and_score_all[:,1])[::-1]\n",
        "  box_and_score_all=box_and_score_all[score_sort]\n",
        "  #print(box_and_score_all)\n",
        "\n",
        " \n",
        "  _,unique_idx=np.unique(box_and_score_all[:,2],return_index=True)\n",
        "  #print(unique_idx)\n",
        "  return box_and_score_all[sorted(unique_idx)]\n",
        "  \n",
        "def NMS(score,y_c,x_c,height,width,iou_thresh,merge_mode=False):\n",
        "  if merge_mode:\n",
        "    score=score\n",
        "    top=y_c\n",
        "    left=x_c\n",
        "    bottom=height\n",
        "    right=width\n",
        "  else:\n",
        "    #flatten\n",
        "    score=score.reshape(-1)\n",
        "    y_c=y_c.reshape(-1)\n",
        "    x_c=x_c.reshape(-1)\n",
        "    height=height.reshape(-1)\n",
        "    width=width.reshape(-1)\n",
        "    size=height*width\n",
        "    \n",
        "    \n",
        "    top=y_c-height/2\n",
        "    left=x_c-width/2\n",
        "    bottom=y_c+height/2\n",
        "    right=x_c+width/2\n",
        "    \n",
        "    inside_pic=(top>0)*(left>0)*(bottom<pred_out_h)*(right<pred_out_w)\n",
        "    outside_pic=len(inside_pic)-np.sum(inside_pic)\n",
        "    #if outside_pic>0:\n",
        "    #  print(\"{} boxes are out of picture\".format(outside_pic))\n",
        "    normal_size=(size<(np.mean(size)*10))*(size>(np.mean(size)/10))\n",
        "    score=score[inside_pic*normal_size]\n",
        "    top=top[inside_pic*normal_size]\n",
        "    left=left[inside_pic*normal_size]\n",
        "    bottom=bottom[inside_pic*normal_size]\n",
        "    right=right[inside_pic*normal_size]\n",
        "  \n",
        "\n",
        "    \n",
        "\n",
        "  #sort  \n",
        "  score_sort=np.argsort(score)[::-1]\n",
        "  score=score[score_sort]  \n",
        "  top=top[score_sort]\n",
        "  left=left[score_sort]\n",
        "  bottom=bottom[score_sort]\n",
        "  right=right[score_sort]\n",
        "  \n",
        "  area=((bottom-top)*(right-left))\n",
        "  \n",
        "  boxes=np.concatenate((score.reshape(-1,1),top.reshape(-1,1),left.reshape(-1,1),bottom.reshape(-1,1),right.reshape(-1,1)),axis=1)\n",
        "  \n",
        "  box_idx=np.arange(len(top))\n",
        "  alive_box=[]\n",
        "  while len(box_idx)>0:\n",
        "  \n",
        "    alive_box.append(box_idx[0])\n",
        "    \n",
        "    y1=np.maximum(top[0],top)\n",
        "    x1=np.maximum(left[0],left)\n",
        "    y2=np.minimum(bottom[0],bottom)\n",
        "    x2=np.minimum(right[0],right)\n",
        "    \n",
        "    cross_h=np.maximum(0,y2-y1)\n",
        "    cross_w=np.maximum(0,x2-x1)\n",
        "    still_alive=(((cross_h*cross_w)/area[0])<iou_thresh)\n",
        "    if np.sum(still_alive)==len(box_idx):\n",
        "      print(\"error\")\n",
        "      print(np.max((cross_h*cross_w)),area[0])\n",
        "    top=top[still_alive]\n",
        "    left=left[still_alive]\n",
        "    bottom=bottom[still_alive]\n",
        "    right=right[still_alive]\n",
        "    area=area[still_alive]\n",
        "    box_idx=box_idx[still_alive]\n",
        "  return boxes[alive_box]#score,top,left,bottom,right\n",
        "\n",
        "\n",
        "\n",
        "def draw_rectangle(box_and_score,img,color):\n",
        "  number_of_rect=np.minimum(500,len(box_and_score))\n",
        "  \n",
        "  for i in reversed(list(range(number_of_rect))):\n",
        "    top, left, bottom, right = box_and_score[i,:]\n",
        "\n",
        "    \n",
        "    top = np.floor(top + 0.5).astype('int32')\n",
        "    left = np.floor(left + 0.5).astype('int32')\n",
        "    bottom = np.floor(bottom + 0.5).astype('int32')\n",
        "    right = np.floor(right + 0.5).astype('int32')\n",
        "    #label = '{} {:.2f}'.format(predicted_class, score)\n",
        "    #print(label)\n",
        "    #rectangle=np.array([[left,top],[left,bottom],[right,bottom],[right,top]])\n",
        "\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    #label_size = draw.textsize(label)\n",
        "    #print(label_size)\n",
        "    \n",
        "    #if top - label_size[1] >= 0:\n",
        "    #  text_origin = np.array([left, top - label_size[1]])\n",
        "    #else:\n",
        "    #  text_origin = np.array([left, top + 1])\n",
        "    \n",
        "    thickness=4\n",
        "    if color==\"red\":\n",
        "      rect_color=(255, 0, 0)\n",
        "    elif color==\"blue\":\n",
        "      rect_color=(0, 0, 255)\n",
        "    else:\n",
        "      rect_color=(0, 0, 0)\n",
        "      \n",
        "    \n",
        "    if i==0:\n",
        "      thickness=4\n",
        "    for j in range(2*thickness):#薄いから何重にか描く\n",
        "      draw.rectangle([left + j, top + j, right - j, bottom - j],\n",
        "                    outline=rect_color)\n",
        "    #draw.rectangle(\n",
        "    #            [tuple(text_origin), tuple(text_origin + label_size)],\n",
        "    #            fill=(0, 0, 255))\n",
        "    #draw.text(text_origin, label, fill=(0, 0, 0))\n",
        "    \n",
        "  del draw\n",
        "  return img\n",
        "            \n",
        "  \n",
        "def check_iou_score(true_boxes,detected_boxes,iou_thresh):\n",
        "  iou_all=[]\n",
        "  for detected_box in detected_boxes:\n",
        "    y1=np.maximum(detected_box[0],true_boxes[:,0])\n",
        "    x1=np.maximum(detected_box[1],true_boxes[:,1])\n",
        "    y2=np.minimum(detected_box[2],true_boxes[:,2])\n",
        "    x2=np.minimum(detected_box[3],true_boxes[:,3])\n",
        "    \n",
        "    cross_section=np.maximum(0,y2-y1)*np.maximum(0,x2-x1)\n",
        "    all_area=(detected_box[2]-detected_box[0])*(detected_box[3]-detected_box[1])+(true_boxes[:,2]-true_boxes[:,0])*(true_boxes[:,3]-true_boxes[:,1])\n",
        "    iou=np.max(cross_section/(all_area-cross_section))\n",
        "    #argmax=np.argmax(cross_section/(all_area-cross_section))\n",
        "    iou_all.append(iou)\n",
        "  score=2*np.sum(iou_all)/(len(detected_boxes)+len(true_boxes))\n",
        "  print(\"score:{}\".format(np.round(score,3)))\n",
        "  return score\n",
        "\n",
        "                \n",
        "\n",
        "\n",
        "\n",
        "for i in np.arange(0,5):\n",
        "  #print(cv_list[i][2:])\n",
        "  img=Image.open(cv_list[i][0]).convert(\"RGB\")\n",
        "  width,height=img.size\n",
        "  predict=model.predict((np.asarray(img.resize((pred_in_w,pred_in_h))).reshape(1,pred_in_h,pred_in_w,3))/255).reshape(pred_out_h,pred_out_w,(category_n+4))\n",
        "  \n",
        "  box_and_score=NMS_all(predict,category_n,score_thresh=0.3,iou_thresh=0.4)\n",
        "\n",
        "  #print(\"after NMS\",len(box_and_score))\n",
        "  if len(box_and_score)==0:\n",
        "    continue\n",
        "\n",
        "  true_boxes=cv_list[i][1][:,1:]#c_x,c_y,width_height\n",
        "  top=true_boxes[:,1:2]-true_boxes[:,3:4]/2\n",
        "  left=true_boxes[:,0:1]-true_boxes[:,2:3]/2\n",
        "  bottom=top+true_boxes[:,3:4]\n",
        "  right=left+true_boxes[:,2:3]\n",
        "  true_boxes=np.concatenate((top,left,bottom,right),axis=1)\n",
        "    \n",
        "  heatmap=predict[:,:,0]\n",
        " \n",
        "  print_w, print_h = img.size\n",
        "  #resize predocted box to original size\n",
        "  box_and_score=box_and_score*[1,1,print_h/pred_out_h,print_w/pred_out_w,print_h/pred_out_h,print_w/pred_out_w]\n",
        "  check_iou_score(true_boxes,box_and_score[:,2:],iou_thresh=0.5)\n",
        "  img=draw_rectangle(box_and_score[:,2:],img,\"red\")\n",
        "  img=draw_rectangle(true_boxes,img,\"blue\")\n",
        "  \n",
        "  fig, axes = plt.subplots(1, 2,figsize=(15,15))\n",
        "  #axes[0].set_axis_off()\n",
        "  axes[0].imshow(img)\n",
        "  #axes[1].set_axis_off()\n",
        "  axes[1].imshow(heatmap)#, cmap='gray')\n",
        "  #axes[2].set_axis_off()\n",
        "  #axes[2].imshow(heatmap_1)#, cmap='gray')\n",
        "  plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GWqBJo4IYqro"
      },
      "source": [
        "Not so bad? The letters in the pictures may be too small considering the output size(heatmap) of 128x128. \n",
        "\n",
        "So let's split pictures into several parts using the results in step1, then run CenterNet for each splitted picture. After then, integrate them and run NMS.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "colab_type": "code",
        "id": "2mTV3cqWJXmR",
        "colab": {}
      },
      "source": [
        "def split_and_detect(model,img,height_split_recommended,width_split_recommended,score_thresh=0.3,iou_thresh=0.4):\n",
        "  width,height=img.size\n",
        "  pred_in_w,pred_in_h=512,512\n",
        "  pred_out_w,pred_out_h=128,128\n",
        "  category_n=1\n",
        "  maxlap=0.5\n",
        "  height_split=int(-(-height_split_recommended//1)+1)\n",
        "  width_split=int(-(-width_split_recommended//1)+1)\n",
        "  height_lap=(height_split-height_split_recommended)/(height_split-1)\n",
        "  height_lap=np.minimum(maxlap,height_lap)\n",
        "  width_lap=(width_split-width_split_recommended)/(width_split-1)\n",
        "  width_lap=np.minimum(maxlap,width_lap)\n",
        "\n",
        "  if height>width:\n",
        "    crop_size=int((height)/(height_split-(height_split-1)*height_lap))#crop_height and width\n",
        "    if crop_size>=width:\n",
        "      crop_size=width\n",
        "      stride=int((crop_size*height_split-height)/(height_split-1))\n",
        "      top_list=[i*stride for i in range(height_split-1)]+[height-crop_size]\n",
        "      left_list=[0]\n",
        "    else:\n",
        "      stride=int((crop_size*height_split-height)/(height_split-1))\n",
        "      top_list=[i*stride for i in range(height_split-1)]+[height-crop_size]\n",
        "      width_split=-(-width//crop_size)\n",
        "      stride=int((crop_size*width_split-width)/(width_split-1))\n",
        "      left_list=[i*stride for i in range(width_split-1)]+[width-crop_size]\n",
        "\n",
        "  else:\n",
        "    crop_size=int((width)/(width_split-(width_split-1)*width_lap))#crop_height and width\n",
        "    if crop_size>=height:\n",
        "      crop_size=height\n",
        "      stride=int((crop_size*width_split-width)/(width_split-1))\n",
        "      left_list=[i*stride for i in range(width_split-1)]+[width-crop_size]\n",
        "      top_list=[0]\n",
        "    else:\n",
        "      stride=int((crop_size*width_split-width)/(width_split-1))\n",
        "      left_list=[i*stride for i in range(width_split-1)]+[width-crop_size]\n",
        "      height_split=-(-height//crop_size)\n",
        "      stride=int((crop_size*height_split-height)/(height_split-1))\n",
        "      top_list=[i*stride for i in range(height_split-1)]+[height-crop_size]\n",
        "  \n",
        "  count=0\n",
        "\n",
        "  for top_offset in top_list:\n",
        "    for left_offset in left_list:\n",
        "      img_crop = img.crop((left_offset, top_offset, left_offset+crop_size, top_offset+crop_size))\n",
        "      predict=model.predict((np.asarray(img_crop.resize((pred_in_w,pred_in_h))).reshape(1,pred_in_h,pred_in_w,3))/255).reshape(pred_out_h,pred_out_w,(category_n+4))\n",
        "  \n",
        "      box_and_score=NMS_all(predict,category_n,score_thresh,iou_thresh)#category,score,top,left,bottom,right\n",
        "      \n",
        "      #print(\"after NMS\",len(box_and_score))\n",
        "      if len(box_and_score)==0:\n",
        "        continue\n",
        "      #reshape and offset\n",
        "      box_and_score=box_and_score*[1,1,crop_size/pred_out_h,crop_size/pred_out_w,crop_size/pred_out_h,crop_size/pred_out_w]+np.array([0,0,top_offset,left_offset,top_offset,left_offset])\n",
        "      \n",
        "      if count==0:\n",
        "        box_and_score_all=box_and_score\n",
        "      else:\n",
        "        box_and_score_all=np.concatenate((box_and_score_all,box_and_score),axis=0)\n",
        "      count+=1\n",
        "  #print(\"all_box_num:\",len(box_and_score_all))\n",
        "  #print(box_and_score_all[:10,:],np.min(box_and_score_all[:,2:]))\n",
        "  if count==0:\n",
        "    box_and_score_all=[]\n",
        "  else:\n",
        "    score=box_and_score_all[:,1]\n",
        "    y_c=(box_and_score_all[:,2]+box_and_score_all[:,4])/2\n",
        "    x_c=(box_and_score_all[:,3]+box_and_score_all[:,5])/2\n",
        "    height=-box_and_score_all[:,2]+box_and_score_all[:,4]\n",
        "    width=-box_and_score_all[:,3]+box_and_score_all[:,5]\n",
        "    #print(np.min(height),np.min(width))\n",
        "    box_and_score_all=NMS(box_and_score_all[:,1],box_and_score_all[:,2],box_and_score_all[:,3],box_and_score_all[:,4],box_and_score_all[:,5],iou_thresh=0.5,merge_mode=True)\n",
        "  return box_and_score_all\n",
        "\n",
        "\n",
        "print(\"test run. 5 image\")\n",
        "all_iou_score=[]\n",
        "for i in np.arange(0,5):\n",
        "  img=Image.open(cv_list[i][0]).convert(\"RGB\")\n",
        "  box_and_score_all=split_and_detect(model,img,cv_list[i][2],cv_list[i][3],score_thresh=0.3,iou_thresh=0.4)\n",
        "  if len(box_and_score_all)==0:\n",
        "    print(\"no box found\")\n",
        "    continue\n",
        "  true_boxes=cv_list[i][1][:,1:]#c_x,c_y,width_height\n",
        "  top=true_boxes[:,1:2]-true_boxes[:,3:4]/2\n",
        "  left=true_boxes[:,0:1]-true_boxes[:,2:3]/2\n",
        "  bottom=top+true_boxes[:,3:4]\n",
        "  right=left+true_boxes[:,2:3]\n",
        "  true_boxes=np.concatenate((top,left,bottom,right),axis=1)\n",
        "\n",
        "  \n",
        "\n",
        " \n",
        "  print_w, print_h = img.size\n",
        "  iou_score=check_iou_score(true_boxes,box_and_score_all[:,1:],iou_thresh=0.5)\n",
        "  all_iou_score.append(iou_score)\n",
        "  \"\"\"\n",
        "  img=draw_rectangle(box_and_score_all[:,1:],img,\"red\")\n",
        "  img=draw_rectangle(true_boxes,img,\"blue\")\n",
        "  \n",
        "  fig, axes = plt.subplots(1, 2,figsize=(15,15))\n",
        "  #axes[0].set_axis_off()\n",
        "  axes[0].imshow(img)\n",
        "  #axes[1].set_axis_off()\n",
        "  axes[1].imshow(heatmap)#, cmap='gray')\n",
        "\n",
        "  plt.show()\n",
        "  \"\"\"\n",
        "print(\"average_score:\",np.mean(all_iou_score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsmrXmDpvM3i",
        "colab_type": "text"
      },
      "source": [
        "OK. Score has improved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E2sHIvRLHSp1"
      },
      "source": [
        "## STEP 3: Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wXgT65k4HSwN"
      },
      "source": [
        "**Goal of stage3 is to classify the letters.**\n",
        "\n",
        "Since this is not main topic of this kernel, I apply simple classification with CNN and skip any pre/postprocessing. I don't care inbalanced data as well.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ePR1mzxkcQtb"
      },
      "source": [
        "### Create Model & Training Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3h5Qn1dUcQ5x"
      },
      "source": [
        "crop all letters in advance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "colab_type": "code",
        "id": "BBfwMtjMHTLr",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "count=0\n",
        "crop_dir=\"/crop_letter/\"\n",
        "if os.path.exists(crop_dir) == False:os.mkdir(crop_dir)\n",
        "\n",
        "train_input=[]\n",
        "pic_count=0\n",
        "for ann_pic in tqdm(annotation_list_train):\n",
        "  pic_count+=1\n",
        "  with Image.open(ann_pic[0]) as img:\n",
        "    for ann in ann_pic[1]:#cat,center_x,center_y,width,height for each picture\n",
        "      cat=ann[0]\n",
        "      c_x=ann[1]\n",
        "      c_y=ann[2]\n",
        "      width=ann[3]\n",
        "      height=ann[4]\n",
        "      save_dir=crop_dir+str(count)+\".jpg\"\n",
        "      img.crop((int(c_x-width/2),int(c_y-height/2),int(c_x+width/2),int(c_y+height/2))).save(save_dir)\n",
        "      train_input.append([save_dir,cat])\n",
        "      count+=1\n",
        "                 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0PbF534toEmL"
      },
      "source": [
        "show example of cropped picture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "colab_type": "code",
        "id": "9nMaUU_-HTTV",
        "colab": {}
      },
      "source": [
        "df_unicode_translation=pd.read_csv(\"../input/unicode_translation.csv\")\n",
        "unicode=df_unicode_translation[\"Unicode\"].values\n",
        "char=df_unicode_translation[\"char\"].values\n",
        "dict_translation={unicode[i]:char[i] for i in range(len(unicode))}\n",
        "\n",
        "i=0\n",
        "img = np.asarray(Image.open(train_input[i][0]).resize((32,32)).convert('RGB'))\n",
        "name = dict_translation[inv_dict_cat[str(train_input[i][1])]]\n",
        "print(name)\n",
        "plt.imshow(img)\n",
        "plt.show()\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OUBUl6dTHTWJ",
        "colab": {}
      },
      "source": [
        "input_height,input_width=32,32\n",
        "\n",
        "def Datagen_for_classification(filenames, batch_size, is_train=True,random_crop=True):\n",
        "  x=[]\n",
        "  y=[]\n",
        "  \n",
        "  count=0\n",
        "\n",
        "  while True:\n",
        "    for i in range(len(filenames)):\n",
        "      if random_crop:\n",
        "        crop_ratio=np.random.uniform(0.8,1)\n",
        "      else:\n",
        "        crop_ratio=1\n",
        "      with Image.open(filenames[i][0]) as f:\n",
        "        \n",
        "        #random crop\n",
        "        if random_crop and is_train:\n",
        "          pic_width,pic_height=f.size\n",
        "          f=np.asarray(f.convert('RGB'),dtype=np.uint8)\n",
        "          top_offset=np.random.randint(0,pic_height-int(crop_ratio*pic_height))\n",
        "          left_offset=np.random.randint(0,pic_width-int(crop_ratio*pic_width))\n",
        "          bottom_offset=top_offset+int(crop_ratio*pic_height)\n",
        "          right_offset=left_offset+int(crop_ratio*pic_width)\n",
        "          f=cv2.resize(f[top_offset:bottom_offset,left_offset:right_offset,:],(input_height,input_width))\n",
        "        else:\n",
        "          f=f.resize((input_width, input_height))\n",
        "          f=np.asarray(f.convert('RGB'),dtype=np.uint8)          \n",
        "        x.append(f)\n",
        "      \n",
        "        y.append(int(filenames[i][1]))\n",
        "      count+=1\n",
        "      if count==batch_size:\n",
        "        x=np.array(x, dtype=np.float32)\n",
        "        y=np.identity(len(dict_cat))[y].astype(np.float32)\n",
        "\n",
        "        inputs=x/255\n",
        "        targets=y       \n",
        "        x=[]\n",
        "        y=[]\n",
        "        count=0\n",
        "        yield inputs, targets\n",
        "        \n",
        "def create_classification_model(input_shape, n_category):\n",
        "    input_layer = Input(input_shape)#32\n",
        "    x=cbr(input_layer,64,3,1)\n",
        "    x=resblock(x,64)\n",
        "    x=resblock(x,64)\n",
        "    x=cbr(input_layer,128,3,2)#16\n",
        "    x=resblock(x,128)\n",
        "    x=resblock(x,128)\n",
        "    x=cbr(input_layer,256,3,2)#8\n",
        "    x=resblock(x,256)\n",
        "    x=resblock(x,256)\n",
        "    x=GlobalAveragePooling2D()(x)\n",
        "    x=Dropout(0.2)(x)\n",
        "    out=Dense(n_category,activation=\"softmax\")(x)#sigmoid???catcrossていぎ\n",
        "    \n",
        "    classification_model=Model(input_layer, out)\n",
        "    \n",
        "    return classification_model\n",
        "      \n",
        "def model_fit_classification(model,train_list,cv_list,n_epoch,batch_size=32):\n",
        "    hist = model.fit_generator(\n",
        "        Datagen_for_classification(train_list,batch_size, is_train=True,random_crop=True),\n",
        "        steps_per_epoch = len(train_list) // batch_size,\n",
        "        epochs = n_epoch,\n",
        "        validation_data=Datagen_for_classification(cv_list,batch_size, is_train=False,random_crop=False),\n",
        "        validation_steps = len(cv_list) // batch_size,\n",
        "        #callbacks = [early_stopping, reduce_lr, model_checkpoint],\n",
        "        shuffle = True,\n",
        "        verbose = 1\n",
        "    )\n",
        "    return hist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-output": true,
        "colab_type": "code",
        "id": "D1iT2UXIM36n",
        "colab": {}
      },
      "source": [
        "K.clear_session()\n",
        "input_height,input_width=32,32\n",
        "model=create_classification_model(input_shape=(input_height,input_width,3),n_category=len(dict_cat))\n",
        "\"\"\"\n",
        "\n",
        "# EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor = 'val_loss', min_delta=0, patience = 60, verbose = 1)\n",
        "# ModelCheckpoint\n",
        "weights_dir = './model_3/'\n",
        "\n",
        "if os.path.exists(weights_dir) == False:os.mkdir(weights_dir)\n",
        "model_checkpoint = ModelCheckpoint(weights_dir + \"val_loss{val_loss:.3f}.hdf5\", monitor = 'val_loss', verbose = 1,\n",
        "                                      save_best_only = True, save_weights_only = True, period = 1)\n",
        "# reduce learning rate\n",
        "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.5, patience = 10, verbose = 1)\n",
        "\"\"\"\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WnSIREedcvpC"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aUcQY7QvM4C",
        "colab_type": "text"
      },
      "source": [
        "Only 10 epoch. Execution time is limited...\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AZ_wbRYUM395",
        "colab": {}
      },
      "source": [
        "train_list, cv_list = train_test_split(train_input, random_state = 111,test_size = 0.2)\n",
        "learning_rate=0.005\n",
        "n_epoch=10\n",
        "batch_size=64\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=learning_rate),metrics=[\"accuracy\"])\n",
        "hist = model_fit_classification(model,train_list,cv_list,n_epoch,batch_size)\n",
        "\n",
        "model.save_weights('final_weights_step3.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "whvk74IYczOc"
      },
      "source": [
        "### Result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WeHypI8Edoo0"
      },
      "source": [
        "Let's check some results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rC5N6PrpM333",
        "colab": {}
      },
      "source": [
        "for i in range(3):\n",
        "  img = np.asarray(Image.open(train_input[i][0]).resize((32,32)).convert('RGB'))\n",
        "  predict=np.argmax(model.predict(img.reshape(1,32,32,3)/255),axis=1)[0]\n",
        "  name = dict_translation[inv_dict_cat[str(predict)]]\n",
        "  print(name)\n",
        "  plt.imshow(img)\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}